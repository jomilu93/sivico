{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ba114c9",
   "metadata": {},
   "source": [
    "# Scoring system with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976a64d",
   "metadata": {},
   "source": [
    "## Summary of the approach in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f22ba",
   "metadata": {},
   "source": [
    "**Text Preprocessing:** We apply several preprocessing steps to the text data to make it easier for our algorithms to work with. This includes transforming all text to lowercase, removing punctuation and numeric values, splitting the text into individual words (tokenization), removing common words (stopwords), and reducing words to their base or root form (lemmatization). The preprocessed data is then saved for further analysis.\n",
    "\n",
    "**Vectorization:** The preprocessed text is transformed into a numerical format using TF-IDF (Term Frequency-Inverse Document Frequency). This results in a matrix where each row represents a senator's initiatives and each column represents a word. The value in each cell is the TF-IDF value of the word in the corresponding document.\n",
    "\n",
    "**Similarity Calculation:** We then use this vectorized data to match a user's interests to the senators' initiatives. This is done by preprocessing and vectorizing the user's input in the same way as the senator's initiatives, and then calculating the cosine similarity between the user's vector and each senator's vector. The cosine similarity provides a score between 0 (completely dissimilar) and 1 (completely similar) for each senator, indicating how closely their initiatives match the user's interests.\n",
    "\n",
    "**Output:** Finally, we rank the senators based on their similarity scores, providing a list of the senators whose initiatives best match the user's input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411dedf",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea9539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import stanza\n",
    "import string\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8523320",
   "metadata": {},
   "source": [
    "## Initialize testing DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709fffe",
   "metadata": {},
   "source": [
    "This dataframe is to test the functions in this notebook with a limited set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde28b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENATORS_TO_PROCESS = 3\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_path)\n",
    "project_data_path = os.path.join(parent_directory, 'data')\n",
    "\n",
    "\n",
    "senators_test_df = pd.read_csv(os.path.join(project_data_path, 'senators_data.csv')).head(SENATORS_TO_PROCESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4462d4",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456387c",
   "metadata": {},
   "source": [
    "### Downloading the Spanish stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71764a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/luis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5870d1c",
   "metadata": {},
   "source": [
    "### Define spanish stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce456aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a638dc6c",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308bee63",
   "metadata": {},
   "source": [
    "### Download spanish tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ffebde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1879771f1254666b86fb9cba9bf06b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 10:37:49 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2023-07-12 10:37:51 INFO: File exists: /Users/luis/stanza_resources/es/default.zip\n",
      "2023-07-12 10:37:54 INFO: Finished downloading models and saved to /Users/luis/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "# downloads tools for processing spanish text\n",
    "stanza.download('es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e000ca",
   "metadata": {},
   "source": [
    "### Initialize Stanza's neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c8290cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 10:37:55 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f461ec5bb404e48b95c18950f2cee27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 10:37:56 INFO: Loading these models for language: es (Spanish):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | ancora   |\n",
      "| mwt          | ancora   |\n",
      "| pos          | ancora   |\n",
      "| lemma        | ancora   |\n",
      "| constituency | combined |\n",
      "| depparse     | ancora   |\n",
      "| sentiment    | tass2020 |\n",
      "| ner          | conll02  |\n",
      "===========================\n",
      "\n",
      "2023-07-12 10:37:56 INFO: Using device: cpu\n",
      "2023-07-12 10:37:56 INFO: Loading: tokenize\n",
      "2023-07-12 10:37:56 INFO: Loading: mwt\n",
      "2023-07-12 10:37:56 INFO: Loading: pos\n",
      "2023-07-12 10:37:56 INFO: Loading: lemma\n",
      "2023-07-12 10:37:56 INFO: Loading: constituency\n",
      "2023-07-12 10:37:57 INFO: Loading: depparse\n",
      "2023-07-12 10:37:57 INFO: Loading: sentiment\n",
      "2023-07-12 10:37:57 INFO: Loading: ner\n",
      "2023-07-12 10:37:58 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Stanza's pipelines contain tools for processing spanish text\n",
    "nlp = stanza.Pipeline('es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98774682",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd290e6",
   "metadata": {},
   "source": [
    "### Preprocess flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee0ed04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\[.*?\\]', '', text) # remove enclosed text i.e. [este texto esta entre llaves]\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text) #remove alphanumeric characters\n",
    "    \n",
    "    # Tokenization and filtering stop words\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    doc = nlp(' '.join(text))\n",
    "    lemmas = []\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            lemmas.append(word.lemma)\n",
    "            \n",
    "    text = ' '.join(lemmas)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b0f18",
   "metadata": {},
   "source": [
    "### Run preprocess flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d53a3",
   "metadata": {},
   "source": [
    "#### Run preprocess flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f20f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing to the 'initiatives_summary_dummy' column\n",
    "senators_test_df['initiatives_summary_preprocessed'] = senators_test_df['initiatives_summary_dummy'].apply(preprocess_text)\n",
    "\n",
    "# Save the preprocessed dataframe\n",
    "senators_test_df.to_csv(os.path.join(project_data_path, 'senators_processed_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936465a",
   "metadata": {},
   "source": [
    "## Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00b45dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.read_csv(os.path.join(project_data_path, 'senators_processed_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c2b8238",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(processed_df['initiatives_summary_preprocessed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6eeac",
   "metadata": {},
   "source": [
    "### Save tfidf_matrix and vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a52bb8",
   "metadata": {},
   "source": [
    "Here's why we need to save both:\n",
    "\n",
    "**TF-IDF Matrix:** This matrix is a numerical representation of our preprocessed text data. Each row corresponds to a senator and each column corresponds to a word. The value in each cell is the TF-IDF value of the word for the corresponding senator. By saving this matrix, we keep a record of how each word is associated with each senator, based on the senator initiatives. We'll use this matrix to compare the user's input with each senator's initiatives.\n",
    "\n",
    "**Fitted Vectorizer:** This is the object that we used to convert our text data into the TF-IDF matrix. It has been 'fitted' to our text data, meaning it has learned the vocabulary of our text data. When we get a new piece of text (the user's input), we'll need to convert it into the same TF-IDF format as our existing data. To do this, we'll use the same vectorizer that we used for our original data. By using the fitted vectorizer, we ensure that the user's input is transformed in the same way as our original text data.\n",
    "\n",
    "In summary, we save the TF-IDF matrix and the fitted vectorizer so that we can use them later to match user input to senator initiatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d79ca4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF-IDF matrix and the fitted vectorizer for later use\n",
    "with open('tfidf_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "\n",
    "with open('fitted_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f0357",
   "metadata": {},
   "source": [
    "## Test user input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6db098",
   "metadata": {},
   "source": [
    "### Load the TF-IDF matrix and the fitted vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dcf511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_matrix.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open('fitted_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c381a",
   "metadata": {},
   "source": [
    "### Function that matches senators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dde04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_senators(user_input, df, vectorizer):\n",
    "    # Preprocess the user's input\n",
    "    user_input_preprocessed = preprocess_text(user_input)\n",
    "\n",
    "    # Transform the preprocessed user input into a TF-IDF vector\n",
    "    user_vector = vectorizer.transform([user_input_preprocessed])\n",
    "\n",
    "    # Calculate the cosine similarity between the user's vector and each senator's vector\n",
    "    similarity_scores = cosine_similarity(user_vector, X)\n",
    "\n",
    "    # Add the similarity scores to the dataframe\n",
    "    df['similarity_score'] = similarity_scores[0]\n",
    "\n",
    "    # Sort the dataframe by similarity score\n",
    "    df_sorted = df.sort_values('similarity_score', ascending=False)\n",
    "\n",
    "    # Return the sorted dataframe\n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180bab33",
   "metadata": {},
   "source": [
    "### Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fcdf658",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Quiero proteccion para los animales\"\n",
    "df_sorted = match_senators(user_input, processed_df, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7122d9",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d87045cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apellidos</th>\n",
       "      <th>Nombre</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rojas Loreto</td>\n",
       "      <td>Estrella</td>\n",
       "      <td>0.107948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Botello Montes</td>\n",
       "      <td>José Alfredo</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Moya Clemente</td>\n",
       "      <td>Roberto Juan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Apellidos        Nombre  similarity_score\n",
       "1    Rojas Loreto      Estrella          0.107948\n",
       "0  Botello Montes  José Alfredo          0.000000\n",
       "2   Moya Clemente  Roberto Juan          0.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted[['Apellidos', 'Nombre', 'similarity_score']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
