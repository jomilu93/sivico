{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mimihuerta/.pyenv/versions/3.10.6/envs/BETOenv/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "! pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presidencia de la República  | 01 de septiembre de 2020  Antes de dar inicio con este evento, se ofrecerá un minuto de silencio en honor a los fallecidos por el COVID. Les pedimos sean tan amables de permanecer de pie.  El presidente de los Estados Unidos Mexicanos se dirige a la escolta de bandera para saludar a nuestro lábaro patrio. Damos inicio a este evento con los honores al presidente constitucional de los Estados Unidos Mexicanos y comandante supremo de las Fuerzas Armadas.  Escuchemos el mensaje que dirige el licenciado Andrés Manuel López Obrador, presidente constitucional de los Estados Unidos Mexicanos con motivo del 2º Informe de Gobierno.  Amigas, amigos: Fui de los primeros en sostener que el principal problema de México era la corrupción y ahora no tengo la menor duda. La peste de la corrupción originó la crisis de México. Por eso, me he propuesto erradicarla por completo y estoy convencido de que, en estos tiempos, más que en otros, transformar es moralizar. Este gobierno no será recordado por corrupto. Nuestro principal legado será purificar la vida pública de México y estamos avanzando. No hemos emprendido persecuciones facciosas ni venganzas políticas, pero tampoco encubrimos a nadie ni permitimos la impunidad. Ya se acabó la robadera de los de arriba, pero todavía falta desterrar por completo el bandidaje oficial. La austeridad republicana es una realidad, son hechos, no palabras. Ya no hay lujos en el gobierno y todo lo que se ahorra se destina a conseguir el bienestar del pueblo. Según cálculos oficiales, por no permitir la corrupción y por hacer un gobierno austero, hemos podido ahorrar durante nuestra administración alrededor de 560 mil millones de pesos. No es para presumir, pero en el peor momento contamos con el mejor gobierno. Estamos enfrentando dos crisis al mismo tiempo: la sanitaria y la económica, y vamos saliendo adelante. La pandemia no es un asunto político, sino de salud pública. Por eso he confiado las decisiones en esta materia a un equipo de profesionales con gran experiencia y capacidad. El coronavirus nos ha dejado dolor, tristeza y penurias, pero también ha fortalecido el amor en las familias, ha demostrado el humanismo y la entrega de los trabajadores de la salud y ha resaltado la conocida fraternidad de nuestro pueblo. Es indudable que saldremos de la pandemia con un mejor sistema de salud. Recibimos el gobierno con 401 hospitales abandonados, saqueados o a medio construir, y con un déficit de más de 200 mil profesionales de la medicina. En pocos meses hemos reconvertido, con el apoyo de los gobiernos estatales, de la Secretaría de Marina y de la Secretaría de la Defensa, 969 hospitales para atender pacientes con COVID-19; se han instalado 32 mil 203 camas y 10 mil 612 con ventiladores. Asimismo, se han contratado 47 mil médicos generales, especialistas, enfermeras y otros trabajadores de la salud. Propusimos ante la ONU, y se aprobó casi por unanimidad, una iniciativa para que los medicamentos y las vacunas tengan carácter no lucrativo. México participa en los principales esfuerzos internacionales para desarrollar una vacuna contra el COVID-19. De manera específica destaco el compromiso que hicimos con la Universidad de Oxford, con el laboratorio AstraZeneca, con la Fundación Carlos Slim y con el gobierno de Argentina. Esperamos que ese acuerdo dé resultados desde noviembre y que podamos empezar a aplicar la vacuna a más tardar a principios del año próximo de manera universal y gratuita. La emergencia sanitaria mundial vivida este año ha planteado problemas muy graves para todos los países y nos obligará a todos a repensar y a cambiar muchas cosas. Además del tremendo dolor que ha causado la pandemia, ha quedado comprobado que sus efectos son más graves si padecemos de hipertensión, de obesidad, de diabetes. Por eso, debemos cuidar nuestra alimentación y no consumir productos chatarra con excesos de azúcares, sales y químicos. Por nuestra salud debemos practicar el ejercicio y el deporte para fortalecer nuestro sistema inmune y bajar de peso. Estamos enfrentando la crisis económica provocada por la pandemia con una fórmula distinta, peculiar, heterodoxa, diría única en el mundo. Ahora, todos los apoyos y créditos se entregan de manera directa para reactivar la economía de abajo hacia arriba, no se da prioridad a las grandes empresas y bancos. Ahora, por el bien de todos, primero se rescata al pueblo. Es motivo de orgullo poder decir que en siete de cada 10 familias está llegando cuando menos un beneficio o algo del presupuesto público, que es dinero de todos. Además, para tranquilidad de nuestra conciencia y felicidad de todas y de todos, el 100 por ciento de las comunidades indígenas y de los más pobres del campo y de la ciudad se benefician con al menos uno de los programas sociales. Aquí recuerdo lo que sostenía Adam Smith, que bien podría constituir uno de los fundamentos de la economía moral que estamos aplicando. Decía: ‘Por más egoísta que quiera suponerse al hombre, evidentemente hay algunos elementos en su naturaleza que lo hacen interesarse en la suerte de los otros, de tal modo que la felicidad de éstos le es necesaria, aunque de ello nada obtenga, a no ser el placer -agregaría, el inmenso placer- de presenciarla’. En otras palabras, la alegría ajena es nuestra propia dicha. No obstante, nos han reprochado que no emprendimos un rescate económico elitista para atenuar los efectos de la pandemia, pero es un timbre de orgullo poder decir que ayudamos por medio de los programas sociales a 23 millones de familias. Imagínense cuántos adultos mayores han podido observar la reclusión sanitaria por contar con el derecho a recibir una pensión, así sea modesta. Ya se reformó el artículo 4º constitucional para convertir los apoyos destinados a los adultos mayores y a niñas y niños con discapacidad, así como el otorgamiento de becas a estudiantes de familias pobres y la atención médica y los medicamentos gratuitos en derechos sociales prioritarios y obligatorios. De ahora en adelante, el gobierno deberá cumplir con este mandato, sea del partido que sea. Este año, debido a la pandemia, los adultos mayores, así como los niños y las niñas con discapacidad, recibieron por anticipado sus pensiones, sólo falta pagar noviembre y diciembre. En ocho meses se han destinado a estos programas 115 mil millones de pesos en beneficio de nueve millones de personas. No es un gasto, sino una inversión; no son dádivas, es justicia. Antes, a los jóvenes se les daba la espalda, se les discriminaba. Ahora tienen garantizado el derecho a la educación y al trabajo, ya no son ‘ninis’, como despectivamente se les decía, ahora son estudiantes becados o aprendices contratados con salario mínimo para que puedan capacitarse y salir adelante. No dejaremos que los jóvenes sean enganchados por la delincuencia, no están solos, con ellos estamos construyendo el futuro. Tres millones de agricultores y pescadores están siendo apoyados, se les otorgan recursos de manera directa, se les ayuda con jornales para cultivar sus tierras, se les entrega fertilizantes de manera gratuita y se establecieron Precios de Garantía para comprar a productores del campo maíz, frijol, arroz, trigo y leche. Es un acto mínimo de justicia. Como decía el poeta Carlos Pellicer, ‘que coman los que nos dan de comer’. Ahora, como dije, todos los apoyos se entregan de manera directa, sin intermediarios. Para ello, se ha fortalecido el Banco del Bienestar con el propósito de que la gente de las comunidades más apartadas pueda recibir lo que le corresponde sin tener que recorrer largas distancias. En 2021 se tendrán dos mil 700 sucursales en todo el país; actualmente, con el apoyo de los ingenieros militares se han construido 226. De igual forma, sigue avanzando el proyecto de Internet para Todos. Ya hay conectividad en 26 mil 789 localidades y en 2021 habrá señal en todo el territorio nacional. Se canceló la mal llamada reforma educativa y ahora caminamos juntos maestros, maestras, madres y padres de familia, estudiantes y autoridades. Se están otorgando 11 millones de becas para alumnas y alumnos pobres de todos los niveles escolares; el gobierno colabora en el mantenimiento de las escuelas y ha quedado claro que la educación no es un privilegio, sino un derecho de todo el pueblo. Gracias, gracias, muchas gracias a nuestros paisanos migrantes. Ahora que más se les ha necesitado es cuando más han ayudado a sus familiares en México. A pesar de la pandemia en Estados Unidos, las remesas han crecido en 10 por ciento en relación con el año pasado y estimo que van a llegar a 40 mil millones de dólares a finales de año, un récord en beneficio de 10 millones de familias. Con el aumento de las remesas que envían nuestros paisanos desde Estados Unidos a sus familiares, con los Programas de Bienestar y los créditos que estamos otorgando a los de abajo, la pandemia no ha desembocado en hambruna ni en escasez de alimentos ni en asaltos, y el pueblo de México tiene recursos para su consumo básico. Pronostiqué que la crisis económica provocada por la pandemia sería transitoria, dije que será como una ‘V’, que caeríamos, pero que saldríamos pronto. Afortunadamente, así está sucediendo: ya pasó lo peor y ahora vamos para arriba, ya se están recuperando los empleos perdidos, se está regresando poco a poco a la normalidad productiva y ya estamos empezando a crecer. En el mes de agosto se crearon 93 mil nuevos empleos. El peso se apreció al cotizarse a menos de 22 pesos por dólar luego de haber estado a más de 25. La mezcla mexicana del petróleo, de cero pasó a 40 dólares por barril. El consumo de productos básicos, en vez de reducirse con la pandemia, aumentó en 9.5 por ciento en términos reales con relación al año pasado. La recaudación de impuestos se mantuvo prácticamente igual que en 2019. Lo mismo sucedió con la inversión extranjera directa, durante el primer semestre del año llegó a 17 mil 969 millones de dólares, lo mismo que en 2019. La caída de la economía, a pesar del desastre mundial, fue de 10.4 por ciento en el semestre; pero, aun con la debacle, fue menor el daño que nos causó la crisis económica que nos está afectando que lo que se está registrando en otros países como Italia, España, Francia y Reino Unido. Debo agregar que casi todos los países recurrieron a créditos y aumentaron sus deudas en porcentajes elevadísimos; en contraste, nosotros hemos enfrentado la pandemia y vamos a salir de la crisis económica sin contratar deuda adicional y sin destinar dinero público a ‘rescates’ -entre comillas- inmorales, es decir, a quienes no necesitan ser rescatados. Pero no debe olvidarse que, al enfocar la solidaridad gubernamental a los más pobres, también beneficiamos indirectamente a los sectores que tienen alguna o mucha capacidad de ahorro. Los programas sociales han permitido a millones de beneficiarios preservar algo de su poder adquisitivo y de su capacidad de consumo, y por eso no se ha cerrado el mercado a miles de empresas y de comercios. La relación con los empresarios ha sido buena y respetuosa. A pesar de la crisis, la mayoría no despidió a sus empleados. Baste un dato: antes de la pandemia estaban inscritos en el Seguro Social 20 millones 500 mil trabajadores en cerca de un millón de empresas, en los momentos más difíciles se perdieron un millón de empleos y ya estamos recuperándonos. ¿Cuántos empleos se mantuvieron a pesar de la pandemia? Diecinueve millones 500 mil empleos. La mayoría de las empresas mantuvo a sus trabajadores. Los empresarios mexicanos cumplen con sus contribuciones, aceptaron aumentar el año pasado 16 por ciento al salario mínimo; este año, 20 por ciento al salario mínimo y decidieron voluntariamente aportar más para pensionar mejor a los trabajadores. Además, los hospitales privados nos han ayudado a enfrentar la pandemia y las televisoras de empresas particulares nos están apoyando para transmitir clases por radio y televisión a 30 millones de estudiantes. No tengo más que decirles: gracias, en nombre del gobierno y de nuestro pueblo. México es un país, sin duda, con porvenir y un ejemplo mundial de cómo hacer realidad el progreso con justicia. La principal riqueza de una nación no está en su infraestructura o en sus finanzas, y ni siquiera en sus recursos naturales, sino en su población y sus culturas, en la gente que la conforma y le da historia y existencia. Invertir en ella, en la población, en los mexicanos, en el pueblo, invertir en su alimentación, su salud, su educación y su bienestar en general es lo mejor que se puede hacer para garantizar la fortaleza del país y su desarrollo presente y futuro. Desde julio pasado entró en vigor el nuevo tratado comercial con Canadá y Estados Unidos. En estos momentos de crisis, el acuerdo que firmamos significa impulsar las actividades productivas, conseguir más inversión extranjera, crear más empleos y lograr más bienestar para nuestro pueblo. En este contexto debe verse mi visita a Washington para entrevistarme con el presidente Donald Trump, quien nos trató con respeto y, lo más importante, elogió a nuestros paisanos que viven y trabajan honradamente en Estados Unidos de América. Mantenemos buenas relaciones con todos los pueblos y gobiernos del mundo. En materia de política exterior nos apegamos a los principios constitucionales de no intervención, autodeterminación de los pueblos, solución pacífica de las controversias y cooperación para el desarrollo. Como es sabido, México fue el país que más votos obtuvo para formar parte del Consejo de Seguridad de la ONU. Con el apoyo de los trabajadores y de los técnicos de Pemex y de la Comisión Federal de Electricidad estamos rescatando a estas empresas públicas, haciéndolas más eficientes, limpiándolas de corrupción y cumpliendo el compromiso de no aumentar el precio de las gasolinas, del diésel, del gas, de la luz y esto lo vamos a sostener durante todo el sexenio. Por convicción hemos decidido cuidar el medio ambiente como nunca lo hicieron los anteriores gobiernos -y, lo más interesante- ni lo demandaron los pseudoecologistas que tanto nos atacan. Sólo subrayo que estamos aplicando el programa de reforestación más importante del mundo sembrando mil 100 millones de árboles frutales y maderables. No se permite el uso del maíz transgénico ni el  , se cuida el agua y no hemos entregado ni una sola concesión para la explotación minera. No se puede olvidar que, en el periodo neoliberal, sólo en 30 años, de 1988 hasta noviembre de 2018, las cinco administraciones pasadas otorgaron concesiones por 118 millones de hectáreas para la explotación minera, el equivalente al 60 por ciento del territorio nacional. Ese entreguismo devastador ya se acabó. Al desterrar la corrupción, el Conacyt pudo orientarse a fortalecer el bienestar del pueblo por medio del desarrollo científico. Así, ha signado becas con transparencia y equidad a 84 mil 599 estudiantes y a 34 mil 447 académicos, con una inversión anual por 19 mil 132 millones de pesos, 14 por ciento más que en 2018. Desde finales de este año nos haremos cargo de que los médicos que quieran especializarse no sean rechazados, como ha venido sucediendo, y puedan formarse tanto en el país como en el extranjero. Estamos contemplando garantizar espacios educativos y entregar durante nuestro gobierno 70 mil becas a estos profesionales de la medicina, que ayudarán a reducir el déficit de especialistas que tiene nuestro país y que nos impide garantizar por completo el derecho del pueblo a la salud. Asimismo, se han sentado las bases de la independencia tecnológica, logrando, en poco tiempo, entre otras aportaciones, el desarrollo y fabricación de los primeros ventiladores 100 por ciento mexicanos para salvar vidas en riesgo por COVID-19. Se han descargado de manera gratuita 683 mil libros digitales del Fondo de Cultura Económica. Tenemos 45 librerías reactivadas de la red Fondo de Cultura Económica Educal, con venta en mostrador y absolutas condiciones sanitarias. Han crecido las salas y los clubes de lectura en comunidades y en escuelas normales. Seguimos produciendo traducciones y nuevos libros. La colección ‘Vientos del Pueblo’ llegará esta semana a 43 títulos, con libros de ocho a 20 pesos en tirajes de 40 mil ejemplares. Tal como nos habíamos comprometido y con miras a la conmemoración el año próximo de los 700 años de la fundación de Tenochtitlan, de los 500 años de la invasión colonial y de los 200 años de la consumación de nuestra Independencia, han comenzado a entrar en imprenta los títulos de la colección ’21 para el 21’, para ser entregados masivamente a partir del año que viene en ediciones de 100 mil ejemplares, producidos por el Fondo de Cultura Económica y financiados por el Instituto para Devolver al Pueblo lo Robado, que incluyen obras de Guillermo Prieto, Elena Poniatowska, Octavio Paz, Emilio Abreu Gómez, Mariano Azuela, Vicente Riva Palacio, Martín Luis Guzmán, José C. Valadés, Luis Villoro, Emilio Carballido, Nellie Campobello, Carlos Monsiváis y Heriberto Frías. Quiero destacar que, por primera vez, una mujer indígena será la titular del Consejo Nacional para Prevenir la Discriminación. También informo que continúa el programa de apoyo preferente a los pueblos originarios. Con ese propósito acabamos de suscribir un acuerdo de justicia con los gobernadores de los pueblos yaquis. Además, seguimos promoviendo el arte y todas las expresiones culturales. Continuamos restaurando templos y monumentos históricos y están en proceso de construcción el Parque Ecológico del Lago de Texcoco y el espacio artístico y cultural de Los Pinos en el Bosque de Chapultepec. Vamos viento en popa en la construcción del aeropuerto ‘Felipe Ángeles’. También está avanzándose, conforme a programa, en la construcción de la nueva refinería de Dos Bocas, Paraíso, Tabasco. Ya se iniciaron los trabajos para el Tren Maya. Estamos haciendo realidad el desarrollo del Istmo de Tehuantepec para comunicar a los países de Asia con la costa este de los Estados Unidos. Además, estas obras generarán 150 mil empleos en el transcurso de este año. Estamos avanzando en el combate a la delincuencia. Hemos establecido una nueva estrategia que empieza por procurar trabajo, educación y bienestar a las personas que están en riesgo de ser reclutadas por los grupos delictivos, especialmente jóvenes. La lucha contra la pobreza, el desempleo y la marginación va acompañada del despliegue de la Guardia Nacional, un cuerpo de paz y de proximidad con la población con presencia en todas las regiones del país, que cuenta ya con 97 mil elementos bien equipados y formados. Para alojar a estos responsables de la seguridad del pueblo se han construido 79 cuarteles, se encuentran en proceso 34 y están por iniciarse 135 más, con lo cual llegaremos a 248 cuarteles a finales del 2021. Casi en todos los delitos ha habido disminución en comparación con noviembre de 2018. Hay menos secuestros, feminicidios, robos a transeúntes, a transportistas, menos robos de vehículos, robo en transporte público colectivo, menos robo en transporte público individual, menos robo a negocios y menos robo a casas habitación. En todos ellos se ha registrado una baja del orden del 30 por ciento en promedio, sólo han aumentado dos delitos: homicidio doloso y extorsión, en 7.9 y 12.7 por ciento respectivamente, vinculados estos delitos, fundamentalmente, a la llamada delincuencia organizada. En esta tarea ha sido fundamental el apoyo profesional y responsable de las Fuerzas Armadas. En particular agradezco el respaldo, la lealtad y el recto proceder del almirante Rafael Ojeda Durán y del general Luis Cresencio Sandoval González, secretarios de Marina y de la Defensa Nacional, servidores públicos ejemplares. Ahora hay justicia para el pobre y en materia de seguridad ya no manda la delincuencia organizada, como era antes. Ya no hay torturas, desapariciones ni masacres, se respetan los derechos humanos y se castiga al culpable, sea quien sea. Ya no hay en el gobierno federal funcionarios como García Luna. He mantenido y seguiré manteniendo una relación institucional con las autoridades emanadas de otros partidos. Las y los gobernadores, y presidentes municipales de cualquier signo político encuentran en el Ejecutivo federal respeto y trato equitativo. Hemos cumplido nuestra promesa de impulsar la verdadera independencia de las instituciones de justicia. La Fiscalía General de la República y el Poder Judicial de la Federación actúan con absoluta autonomía y se acabó aquello de que todo lo ordenaba el presidente porque el Ejecutivo era el poder de los poderes. Miren cómo han cambiado las cosas. Invité al fiscal general de la República y al presidente de la Suprema Corte de Justicia y no pudieron asistir. En otros tiempos eso no pasaba, porque ellos tienen la arrogancia de sentirse libres. Este es el cambio, esta es la transformación. En los casos en los que están implicados expresidentes de la República he propuesto que las autoridades responsables desahoguen el asunto con absoluta libertad y que, de ser necesario, se celebre una consulta para conocer la opinión del pueblo. He dicho y reitero que yo votaría por no someterlos a proceso, pues mantengo la postura que sostuve desde mi toma de posesión, según la cual en el terreno de la justicia se puede castigar los errores del pasado, pero lo fundamental es evitar los delitos del porvenir. Sin embargo, de realizarse la consulta respetaré el fallo popular, sea cual sea, porque en la democracia el pueblo decide y por convicción me he propuesto mandar obedeciendo. Como muchos mexicanos, fui víctima del fraude electoral y estoy convencido del tremendo daño que ocasionan las autoridades impuestas. Por eso se reformó la Constitución y se castigará con cárcel y sin derecho a fianza a quien utilice el presupuesto público en beneficio de partidos y candidatos o adultere el carácter libre y democrático de las elecciones. Desde Francisco I. Madero nunca un presidente había sido tan atacado como ahora. Los conservadores están enojados porque ya no hay corrupción y perdieron privilegios; sin embargo, gozan de una absoluta libertad de expresión y ello es prueba de que hoy se garantizan las libertades y el derecho a disentir. La represión política ha quedado en el pasado. Estamos llevando a cabo la Cuarta Transformación de la vida pública de México y es pertinente recordar que las tres primeras transformaciones, la Independencia, la Reforma y la Revolución tuvieron que hacerse con las armas. Ahora lo estamos logrando de manera pacífica. Hay oposición al gobierno, como debe de existir en toda auténtica democracia, pero la mayoría de los habitantes de México aprueban nuestra gestión. Gracias les doy a todas y todos por la confianza. No le fallaré al pueblo de México. Amigas y amigos: Hoy por la tarde, como lo establece la ley, la licenciada Olga Sánchez Cordero, secretaria de Gobernación, entregará al Congreso de la Unión el Segundo Informe del Gobierno que represento. Es mucho lo realizado. Miren, en lo fundamental nos queda realmente poco por definir. De los 100 compromisos que hice el Zócalo el 1º de diciembre de 2018, hemos cumplido 95 y sólo están pendientes cinco compromisos, o en proceso de que se cumplan. Desde el primer día de mi gobierno se ha venido aplicando el proyecto de nación que propusimos a la sociedad y que obtuvo un respaldo abrumador en las urnas el 1º de julio de 2018. Aunque circunstancias imprevistas e infortunadas, como la pandemia de COVID-19, nos obligan a hacer ajustes, no vamos a apartarnos en lo esencial del espíritu, del compromiso adquirido. Las acciones gubernamentales realizadas son expresión de lo que hemos soñado, diseñado y ofrecido desde hace muchos años, corresponden a una visión de país y a una visión de lo que debe de ser un mundo justo y fraterno. Hoy, algunos críticos piden que se gobierne en sentido distinto, que prescindamos de nuestro ideario y de nuestro proyecto, que apliquemos recetas económicas contra las que hemos luchado o que seamos tolerantes con la corrupción que nos propusimos erradicar. Piden, en suma, que yo traicione mi compromiso con la sociedad, que falte a mi palabra y que renuncie a mi congruencia, y eso, lógicamente, no va a ocurrir. Ya está en marcha la nueva política económica sustentada en la moralidad, la austeridad y el desarrollo con justicia. Sigue en pie el compromiso de terminar de sentar las bases del México del porvenir para el 1º de diciembre próximo, cuando se cumplan dos años de gobierno. A partir de entonces, una vez que se tengan construidos los cimientos, sólo quedará la tarea de terminar la obra de transformación y seguir gobernando con rectitud y amor al pueblo para contar siempre con su respaldo. Estoy convencido que la mejor manera de evitar retrocesos en el futuro depende mucho de continuar con la revolución de las consciencias para lograr a plenitud un cambio de mentalidad que, cuando sea necesario, se convierta en voluntad colectiva dispuesta a defender lo alcanzado en beneficio del interés público y de la nación. Sigamos pues, haciendo historia; sigamos pues, haciendo patria, por nosotros y por las nuevas generaciones que sabrán honrar la dignidad de nuestro pueblo y la grandeza de México. ¡Que viva México! ¡Viva México! ¡Viva México!  Finaliza este evento con los honores al presidente constitucional de los Estados Unidos Mexicanos, y comandante supremo de las Fuerzas Armadas. Se les invita a entonar nuestro Himno Nacional.  El presidente de la República se dirige a la escolta de bandera para despedirse de nuestra Enseña Nacional. ---   VE 1222. Puedes seleccionar más de uno. Puedes seleccionar más de una opción. Gracias por tu opinión. \n",
      "          La legalidad, veracidad y la calidad de la información es estricta responsabilidad de la dependencia, entidad o empresa productiva del Estado que la proporcionó en virtud de sus atribuciones y/o facultades normativas.\n",
      "         Es el portal único de trámites, información y participación ciudadana.   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n7/388dqz5d5fd0c4nd5dh86cbh0000gp/T/ipykernel_25741/3282400625.py:7: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  text = soup.find_all(text=True)\n"
     ]
    }
   ],
   "source": [
    "# Bajar texto de 2do informe de gobierno AMLO\n",
    "\n",
    "url = 'https://www.gob.mx/presidencia/es/articulos/version-estenografica-2-informe-de-gobierno-2019-2020?idiom=es'\n",
    "res = requests.get(url)\n",
    "html_page = res.content\n",
    "soup = BeautifulSoup(html_page, 'html.parser')\n",
    "text = soup.find_all(text=True)\n",
    "\n",
    "# Nos quedamos solo con las etiquetas del texto principal\n",
    "output = ''\n",
    "list = [\n",
    "    'p',\n",
    "]\n",
    "\n",
    "for t in text:\n",
    "    if t.parent.name in list:\n",
    "        output += '{} '.format(t)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = type(output)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26012"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='mrm8488/bert2bert_shared-spanish-finetuned-summarization', vocab_size=31002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/bert2bert_shared-spanish-finetuned-summarization\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "Some weights of EncoderDecoderModel were not initialized from the model checkpoint at mrm8488/bert2bert_shared-spanish-finetuned-summarization and are newly initialized: ['decoder.bert.encoder.layer.5.attention.self.query.bias', 'decoder.bert.encoder.layer.11.attention.output.dense.bias', 'decoder.bert.encoder.layer.1.attention.self.value.bias', 'decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.11.attention.self.key.weight', 'decoder.bert.encoder.layer.7.output.dense.weight', 'decoder.bert.encoder.layer.4.output.dense.bias', 'decoder.bert.encoder.layer.1.attention.self.query.bias', 'decoder.bert.encoder.layer.9.attention.output.dense.weight', 'decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.0.intermediate.dense.bias', 'decoder.bert.encoder.layer.7.attention.self.key.weight', 'decoder.bert.encoder.layer.6.intermediate.dense.weight', 'decoder.bert.encoder.layer.9.attention.self.value.bias', 'decoder.bert.encoder.layer.7.attention.self.query.weight', 'decoder.bert.encoder.layer.7.intermediate.dense.weight', 'decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.6.output.LayerNorm.bias', 'decoder.bert.encoder.layer.10.attention.output.dense.weight', 'decoder.bert.encoder.layer.4.intermediate.dense.bias', 'decoder.cls.predictions.decoder.weight', 'decoder.bert.encoder.layer.3.attention.self.value.bias', 'decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.5.output.dense.bias', 'decoder.bert.encoder.layer.3.attention.self.query.weight', 'decoder.bert.encoder.layer.8.attention.output.dense.weight', 'decoder.bert.encoder.layer.5.attention.self.value.weight', 'decoder.bert.encoder.layer.11.intermediate.dense.weight', 'decoder.bert.encoder.layer.4.output.LayerNorm.bias', 'decoder.bert.encoder.layer.2.output.LayerNorm.bias', 'decoder.bert.encoder.layer.5.attention.output.dense.bias', 'decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.9.output.dense.weight', 'decoder.bert.encoder.layer.0.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.output.LayerNorm.weight', 'decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.9.attention.self.query.weight', 'decoder.bert.encoder.layer.7.output.LayerNorm.weight', 'decoder.bert.encoder.layer.5.output.LayerNorm.weight', 'decoder.bert.encoder.layer.10.attention.self.query.bias', 'decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.3.output.dense.weight', 'decoder.bert.encoder.layer.2.attention.self.value.bias', 'decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.9.attention.output.dense.bias', 'decoder.bert.encoder.layer.3.output.LayerNorm.bias', 'decoder.bert.encoder.layer.6.attention.self.query.weight', 'decoder.bert.encoder.layer.1.attention.self.key.bias', 'decoder.bert.encoder.layer.9.output.dense.bias', 'decoder.bert.encoder.layer.7.attention.self.value.weight', 'decoder.bert.encoder.layer.11.output.LayerNorm.weight', 'decoder.bert.encoder.layer.10.attention.self.query.weight', 'decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.4.attention.self.query.weight', 'decoder.bert.encoder.layer.10.attention.self.key.weight', 'decoder.bert.encoder.layer.4.attention.output.dense.bias', 'decoder.bert.encoder.layer.3.attention.output.dense.weight', 'decoder.bert.encoder.layer.0.output.dense.weight', 'decoder.bert.encoder.layer.4.attention.self.value.bias', 'decoder.bert.encoder.layer.11.intermediate.dense.bias', 'decoder.bert.encoder.layer.6.attention.self.key.weight', 'decoder.bert.encoder.layer.3.attention.self.value.weight', 'decoder.bert.encoder.layer.8.output.dense.weight', 'decoder.bert.encoder.layer.8.output.dense.bias', 'decoder.bert.encoder.layer.6.output.LayerNorm.weight', 'decoder.bert.embeddings.LayerNorm.weight', 'decoder.bert.encoder.layer.10.attention.self.value.bias', 'decoder.bert.encoder.layer.0.attention.self.key.bias', 'decoder.cls.predictions.decoder.bias', 'decoder.bert.encoder.layer.1.output.dense.bias', 'decoder.bert.embeddings.word_embeddings.weight', 'decoder.bert.encoder.layer.8.attention.self.query.bias', 'decoder.bert.encoder.layer.6.output.dense.weight', 'decoder.bert.encoder.layer.1.attention.self.key.weight', 'decoder.bert.encoder.layer.2.attention.self.query.weight', 'decoder.bert.encoder.layer.4.attention.self.key.weight', 'decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.9.output.LayerNorm.bias', 'decoder.bert.encoder.layer.9.output.LayerNorm.weight', 'decoder.bert.encoder.layer.7.attention.output.dense.bias', 'decoder.bert.encoder.layer.0.attention.self.key.weight', 'decoder.bert.encoder.layer.2.output.LayerNorm.weight', 'decoder.bert.encoder.layer.2.attention.self.key.weight', 'decoder.bert.encoder.layer.8.intermediate.dense.bias', 'decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.9.intermediate.dense.bias', 'decoder.bert.encoder.layer.7.attention.self.query.bias', 'decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.10.output.LayerNorm.weight', 'decoder.bert.encoder.layer.6.attention.output.dense.bias', 'decoder.bert.encoder.layer.4.attention.self.value.weight', 'decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.3.attention.self.query.bias', 'decoder.bert.encoder.layer.5.attention.self.value.bias', 'decoder.bert.encoder.layer.10.attention.self.key.bias', 'decoder.bert.encoder.layer.5.attention.self.key.bias', 'decoder.bert.encoder.layer.8.attention.output.dense.bias', 'decoder.bert.encoder.layer.9.attention.self.value.weight', 'decoder.bert.encoder.layer.10.output.dense.bias', 'decoder.bert.encoder.layer.8.attention.self.key.bias', 'decoder.bert.encoder.layer.7.attention.self.value.bias', 'decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.intermediate.dense.weight', 'decoder.bert.encoder.layer.2.attention.self.query.bias', 'decoder.bert.encoder.layer.6.attention.self.query.bias', 'decoder.bert.encoder.layer.3.intermediate.dense.weight', 'decoder.bert.encoder.layer.11.output.dense.weight', 'decoder.bert.encoder.layer.3.attention.self.key.weight', 'decoder.bert.encoder.layer.9.intermediate.dense.weight', 'decoder.bert.encoder.layer.5.attention.self.query.weight', 'decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.11.output.dense.bias', 'decoder.bert.encoder.layer.4.attention.output.dense.weight', 'decoder.bert.encoder.layer.0.attention.self.query.weight', 'decoder.bert.encoder.layer.11.attention.output.dense.weight', 'decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.7.output.LayerNorm.bias', 'decoder.bert.encoder.layer.9.attention.self.query.bias', 'decoder.bert.encoder.layer.0.intermediate.dense.weight', 'decoder.bert.encoder.layer.9.attention.self.key.bias', 'decoder.bert.encoder.layer.2.attention.output.dense.weight', 'decoder.bert.encoder.layer.1.output.LayerNorm.weight', 'decoder.bert.encoder.layer.11.attention.self.key.bias', 'decoder.bert.encoder.layer.0.attention.output.dense.bias', 'decoder.bert.encoder.layer.3.intermediate.dense.bias', 'decoder.bert.encoder.layer.10.intermediate.dense.bias', 'decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.5.attention.output.dense.weight', 'decoder.bert.encoder.layer.3.output.dense.bias', 'decoder.bert.encoder.layer.8.intermediate.dense.weight', 'decoder.bert.encoder.layer.0.output.dense.bias', 'decoder.bert.encoder.layer.5.intermediate.dense.bias', 'decoder.bert.encoder.layer.7.output.dense.bias', 'decoder.bert.encoder.layer.7.attention.self.key.bias', 'decoder.bert.encoder.layer.11.attention.self.query.weight', 'decoder.bert.encoder.layer.1.attention.self.query.weight', 'decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.8.attention.self.query.weight', 'decoder.bert.encoder.layer.6.attention.self.value.weight', 'decoder.bert.encoder.layer.0.attention.output.dense.weight', 'decoder.bert.encoder.layer.6.intermediate.dense.bias', 'decoder.bert.encoder.layer.3.attention.self.key.bias', 'decoder.bert.encoder.layer.11.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.attention.self.query.bias', 'decoder.bert.encoder.layer.6.attention.self.key.bias', 'decoder.bert.encoder.layer.11.attention.self.query.bias', 'decoder.bert.encoder.layer.6.attention.self.value.bias', 'decoder.bert.encoder.layer.5.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.attention.self.key.bias', 'decoder.bert.encoder.layer.4.output.dense.weight', 'decoder.bert.encoder.layer.8.attention.self.value.bias', 'decoder.bert.encoder.layer.2.intermediate.dense.weight', 'decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.0.attention.self.value.weight', 'decoder.bert.encoder.layer.10.attention.self.value.weight', 'decoder.bert.encoder.layer.0.attention.self.value.bias', 'decoder.bert.encoder.layer.10.output.dense.weight', 'decoder.bert.encoder.layer.1.attention.output.dense.weight', 'decoder.bert.encoder.layer.2.attention.output.dense.bias', 'decoder.bert.encoder.layer.2.attention.self.value.weight', 'decoder.bert.encoder.layer.7.attention.output.dense.weight', 'decoder.bert.encoder.layer.3.output.LayerNorm.weight', 'decoder.bert.encoder.layer.3.attention.output.dense.bias', 'decoder.bert.encoder.layer.0.output.LayerNorm.weight', 'decoder.bert.encoder.layer.7.intermediate.dense.bias', 'decoder.bert.embeddings.token_type_embeddings.weight', 'decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.8.output.LayerNorm.bias', 'decoder.bert.encoder.layer.10.attention.output.dense.bias', 'decoder.bert.encoder.layer.8.output.LayerNorm.weight', 'decoder.bert.encoder.layer.11.attention.self.value.bias', 'decoder.bert.encoder.layer.2.output.dense.bias', 'decoder.bert.encoder.layer.0.attention.self.query.bias', 'decoder.bert.encoder.layer.1.attention.output.dense.bias', 'decoder.bert.encoder.layer.1.intermediate.dense.bias', 'decoder.bert.encoder.layer.1.intermediate.dense.weight', 'decoder.bert.encoder.layer.2.intermediate.dense.bias', 'decoder.bert.encoder.layer.9.attention.self.key.weight', 'decoder.bert.encoder.layer.6.attention.output.dense.weight', 'decoder.bert.encoder.layer.5.output.dense.weight', 'decoder.bert.encoder.layer.1.output.dense.weight', 'decoder.bert.encoder.layer.10.intermediate.dense.weight', 'decoder.bert.encoder.layer.6.output.dense.bias', 'decoder.bert.encoder.layer.8.attention.self.key.weight', 'decoder.bert.embeddings.position_embeddings.weight', 'decoder.bert.encoder.layer.1.attention.self.value.weight', 'decoder.bert.encoder.layer.1.output.LayerNorm.bias', 'decoder.bert.encoder.layer.8.attention.self.value.weight', 'decoder.bert.embeddings.LayerNorm.bias', 'decoder.bert.encoder.layer.10.output.LayerNorm.bias', 'decoder.bert.encoder.layer.5.attention.self.key.weight', 'decoder.bert.encoder.layer.2.attention.self.key.bias', 'decoder.bert.encoder.layer.11.attention.self.value.weight', 'decoder.bert.encoder.layer.2.output.dense.weight', 'decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.5.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoderModel(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=31002, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/bert2bert_shared-spanish-finetuned-summarization\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    4, 12159, 10368,  2488,  1181,  7783,  1008,  1030,  2693,   942,\n",
       "          4270,  1008,  3112,  1008, 20109,  5157,  1008,  2166,  6471,  1051,\n",
       "          1365,  7901,  1017,  1062, 30648,  1049,  5161,  1008,  8043,  1036,\n",
       "          4620,  1013,  1065, 17731, 30934,  1096,  1040,  2396,  8895, 30971,\n",
       "          1009,  3958, 17572,  3702,  1431, 22391,  1008,  8789,  1008,  2844,\n",
       "          1009,  1162,  3599,  1008,  1065,  1636,  2648, 15912,  2042,  1062,\n",
       "         10493,  1013,  1030, 24742,  1008, 10498,  1110, 17036,  1013,  2294,\n",
       "         10149,  1757, 30933,  8367, 30933,  1009, 14460,  1011,  6471,  1013,\n",
       "          1365,  7901,  1051,  1065, 26406,  1091,  3599, 12407,  1008,  1065,\n",
       "          1636,  2648, 15912,  2042,  1042,  9180, 24744,  1008,  1089, 12174,\n",
       "         19349,  1009, 15203,  1305,  1040,  4837,  1038, 10493,  1040, 29365,\n",
       "         13956,  7653, 10804, 23839,  2230,  1017,  3599, 12407,  1008,  1065,\n",
       "          1636,  2648, 15912,  2042,  1051,  5140,  1072,     3,  5416,  1008,\n",
       "          2684,  1009,  9910,  1723,  1017,  3200,  1181,  6508,  1008,  1065,\n",
       "          4474,  1036, 15395,  1038,  1040,  3281,  2492,  1008,  3944,  1538,\n",
       "          1030,  9688,  1042,  1649,  1084,  1847,  1030,  4676,  5247,  1009,\n",
       "          1198, 21054,  1008,  1030,  9688, 30635,  1030,  4816,  1008,  3944,\n",
       "          1009,  1278,  1400,  1017,  1129,  1734,  9238, 20161,  1121,  1096,\n",
       "          5219,  1042,  1764, 14161,  1008,  1038,  1017,  1036,  2111,  5910,\n",
       "          1017,  1216,  1038,  1036,  1759,  1017, 19521,  1058,  9432,  6596,\n",
       "          1009,  2072,  2966,  1084,  2218, 22727,  1096, 17875,  1071,  1009,\n",
       "          6892,  3281, 17543,  2218, 19665,  6093,  1030,  1688,  3863,  1008,\n",
       "          3944,  1042,  2527, 19407,  1009,  1125,  2972, 26157,  1408,  5282,\n",
       "          1332,  3817, 20683,  1382, 11528, 30934,  3350,  1017,  1355,  4705,\n",
       "         24278,  1305,  1013,  2635,  1382, 21436,  1305,  1030, 18419,  1009,\n",
       "          1773,  1062,  6590,  1030, 27077,  2092,  1008,  1065,  1008,  4102,\n",
       "          1017,  1355,  3329,  3389, 30367,  2124,  1096,  5219,  1040,  2222,\n",
       "          2463,  1323,  3844,  1009,  1198,  8466, 29088, 12999,  1481,  1058,\n",
       "          1108,  3169,  1017,  1404,  5579,  1017,  1084,  3836,  1009,  1773,\n",
       "          1084,  1423, 13302, 30934,  1036,  1040,  2966,  1042,  1397,  1114,\n",
       "          1038,  1062,  7914,  1048,  1062, 10697,  1013,  3784,  1040,  9447,\n",
       "          1072,  3094,  1009,  3678, 16066,  5312,  1017,  1096,  1084,  4710,\n",
       "          1030,  9688,  1042,  1096,  1409,  1049,  2966,  8466,  1120,  1017,\n",
       "          2972,  6496, 16727,  1787,  2367,  4519,  4104,  1008,  5264, 30965,\n",
       "          4129,  2439,  1008, 16038,  1009,  1125,  1058,  1110, 29619, 30936,\n",
       "          1017,  1355,  1036,  1040,  5064,  1989, 27921,  1051,  1040,  1627,\n",
       "          2966,  1009,  3437, 15923,  1047,  1471,  4816,  1091,  1698,  1577,\n",
       "          1181,  1030, 12602,  1042,  1030,  3971,  1017,  1042,  2229, 10198,\n",
       "          4286,  1009,  1198,  3231,  7007,  1084,  1058,  1049,  4320,  4593,\n",
       "          1017,  2825,  1008,  3058,  3863,  1009,  1278,  1400,  1734, 24615,\n",
       "          1089,  4222,  1036,  1359,  3043,  1013,  1049,  2520,  1008,  5560,\n",
       "          1051,  1523,  4116,  1042,  3201,  1009,  1162, 12078,  2388,  1212,\n",
       "          1445,  1131,  6306,  4718,  1017, 18658,  1042,  2928, 16379,  1017,\n",
       "          1355,  1447,  1131,  4572,  5283,  1040,  2807,  1036,  1089,  5450,\n",
       "          1017,  1131,  9444,  1040,  1881,  1898,  1042,  1030,  7217,  1008,\n",
       "          1065,  3426,  1008,  1030,  3058,  1042,  1131, 15691,  1419,  1030,\n",
       "          6347, 28239,  1008,  2294,  3094,  1009,  1177, 19985, 23738,  1038,\n",
       "          1435,  5791,  1008,  1030,  3231,  7007,  1051,  1049,  1627,  2116,\n",
       "          1008,  3058,  1009,  6865, 16544,  1040,  2966,  1051,  3242, 30967,\n",
       "         14486, 29174,  1017, 11928,  1168,  1074,  1013,  2181,  6817,  1017,\n",
       "          1042,  1051,  1049, 11061,  1008,  1216,  1008,  1272,  4129,  5560,\n",
       "          1008,     5]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"RESUMEN: \" + output, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumen = model.generate(\n",
    "    inputs, \n",
    "    max_length=300, \n",
    "    min_length=40, \n",
    "    length_penalty=2.0, \n",
    "    num_beams=4, \n",
    "    early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    4,  1198,  8466, 29088, 12999,  1481,  1058,  1108,  3169,  1017,\n",
      "          1404,  5579,  1017,  1084,  3836,  1009,  1162,  3599,  1008,  1065,\n",
      "          1636,  2648,  1062, 10493,  1013,  1030, 24742,  1008, 10498,  1110,\n",
      "         17036,  1013,  1065, 17731, 30934,  1096,  1040,  2396,  8895, 30971,\n",
      "             5]])\n"
     ]
    }
   ],
   "source": [
    "print(resumen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] La austeridad republicana es una realidad, son hechos, no palabras. El presidente de los Estados Unidos se dirige a la escolta de bandera para saludar a los fallecidos por el COVID [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(resumen[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
