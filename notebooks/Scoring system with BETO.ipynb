{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ba114c9",
   "metadata": {},
   "source": [
    "# Scoring system with BETO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9e177",
   "metadata": {},
   "source": [
    "## Summary of the approach in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed679e2",
   "metadata": {},
   "source": [
    "**Choice of BETO:** We choose BETO, a BERT model trained on Spanish corpora, because our text data is in Spanish and we need a model that understands the semantic meaning of sentences. BERT models are great at this because they're designed to understand the context of words in a sentence. By using BETO, we capture more complex language features compared to simpler techniques like TF-IDF.\n",
    "\n",
    "**Generating Embeddings:** We use BETO to generate embeddings for both the senators' initiative profiles and the user's input. These embeddings are vectors in a high-dimensional space that represent the semantic meaning of the text. By representing the text as vectors, we can calculate the distance (or similarity) between different pieces of text.\n",
    "\n",
    "**Calculating Similarity:** To match the user's input to the senators' profiles, we calculate the cosine similarity between the user's input vector and each senator's vector. This gives us a measure of how similar the user's input is to each senator's profile, which we use as our scoring mechanism.\n",
    "\n",
    "**Ranking Senators:** Finally, we rank the senators based on their similarity scores and return the top N senators. This gives us a list of senators whose initiative profiles best match the user's interests.\n",
    "\n",
    "The core idea of this approach is to leverage the power of language models like BETO to understand the semantic meaning of text and use this to match users with senators based on their interests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411dedf",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea9539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6051cac",
   "metadata": {},
   "source": [
    "## Initialize testing DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d5d85",
   "metadata": {},
   "source": [
    "This dataframe is to test the functions in this notebook with a limited set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a48029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENATORS_TO_PROCESS = 3\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_path)\n",
    "project_data_path = os.path.join(parent_directory, 'data')\n",
    "\n",
    "\n",
    "senators_test_df = pd.read_csv(os.path.join(project_data_path, 'senators_data.csv')).head(SENATORS_TO_PROCESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b7f73",
   "metadata": {},
   "source": [
    "## Download BETO model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007720b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "model = BertModel.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6321210",
   "metadata": {},
   "source": [
    "### Test BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05251d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0733, -0.2156,  0.1107,  ..., -0.1891,  0.1999,  0.1254],\n",
       "         [-0.0935,  0.1006, -0.4794,  ..., -0.1307,  0.3479,  0.3917],\n",
       "         [-0.2780,  1.1009, -0.6256,  ...,  0.0978,  0.3362,  0.4117],\n",
       "         [-0.5546, -0.0858, -0.3468,  ...,  0.1149,  0.3315, -0.0632],\n",
       "         [-0.4995, -0.1995, -0.0398,  ..., -0.3834,  0.0416, -0.3520],\n",
       "         [-0.6204, -0.1591, -0.4836,  ..., -0.8248,  0.9292, -0.4672]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "inputs = tokenizer(\"Arriba la democracia!\", return_tensors=\"pt\")\n",
    "\n",
    "# Generate the embeddings\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# The embeddings are stored in the `last_hidden_state` attribute\n",
    "embeddings = outputs.last_hidden_state\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39735043",
   "metadata": {},
   "source": [
    "## Generate embeddings with BETO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d198bb7",
   "metadata": {},
   "source": [
    "Embeddings are a way to represent text (or other types of data) as vectors of numbers. The key idea behind embeddings is to represent words or sentences in a high-dimensional space in such a way that their location in this space captures some of the semantic meaning of the text.\n",
    "\n",
    "For example, in a well-constructed embedding space, words or sentences with similar meanings will be located near each other, and their relative locations can capture some of the relationships between them. For instance, the vectors for \"king\" and \"queen\" might be located at similar positions in the embedding space, and the direction from \"king\" to \"queen\" might be the same as the direction from \"man\" to \"woman\", capturing the relationship of gender between these words.\n",
    "\n",
    "We use embeddings in NLP (Natural Language Processing) because they provide a way to turn text into a form that machine learning algorithms can understand. Most machine learning algorithms require numerical input, and embeddings provide a way to turn text into numbers while preserving some of the semantic meaning of the text.\n",
    "\n",
    "In the context of this project, we use embeddings to represent both the user's input and the senator's initiative profiles. By representing these texts as vectors in a high-dimensional space, we can calculate the distance (or similarity) between the user's input and each senator's profile. This allows us to rank the senators based on how similar their profile is to the user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea4694b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text, tokenizer, model, max_length=512):\n",
    "    # Split the text into chunks to handle long summaries\n",
    "    text_chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    \n",
    "    # Initialize an empty tensor to store the embeddings\n",
    "    embeddings = torch.zeros((len(text_chunks), model.config.hidden_size))\n",
    "    \n",
    "    # Generate embeddings for each chunk\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        embeddings[i] = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    # Average the embeddings of all the chunks\n",
    "    embeddings = embeddings.mean(dim=0)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44d99f",
   "metadata": {},
   "source": [
    "### Test embeddings function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea14bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [tensor(-0.2797, grad_fn=<UnbindBackward0>), t...\n",
       "1    [tensor(-0.2587, grad_fn=<UnbindBackward0>), t...\n",
       "2    [tensor(-0.2833, grad_fn=<UnbindBackward0>), t...\n",
       "Name: initiatives_summary_dummy, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senators_test_df['initiatives_summary_dummy'].apply(lambda x: generate_embeddings(x, tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a30fb2f",
   "metadata": {},
   "source": [
    "### Why we don't use stop words or lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89ca3cc",
   "metadata": {},
   "source": [
    "In traditional NLP tasks, lemmatization and removing stop words are common steps to reduce the dimensionality of the data and focus on the most informative words. However, BERT-like models are based on transformers that use the context of words in a sentence to understand their meaning. They can even leverage the information contained in stop words. So, for these models, we usually keep the original form of the words and do not remove stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700c9c25",
   "metadata": {},
   "source": [
    "## Score senator profiles based on user input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077031de",
   "metadata": {},
   "source": [
    "### Function to get scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35fd9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define a function to match senators based on the user's input\n",
    "def match_senators(user_input, senators_embeddings, tokenizer, model):\n",
    "    # Generate an embedding for the user's input\n",
    "    user_embedding = generate_embeddings(user_input, tokenizer, model)\n",
    "\n",
    "    # Initialize an empty array to store the similarity scores\n",
    "    scores = np.zeros(len(senators_embeddings))\n",
    "\n",
    "    # Calculate the cosine similarity between the user's input and each senator's profile\n",
    "    for i, senator_embedding in enumerate(senators_embeddings):\n",
    "        scores[i] = cosine_similarity(user_embedding.detach().numpy().reshape(1, -1), senator_embedding.detach().numpy().reshape(1, -1))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dfbfc4",
   "metadata": {},
   "source": [
    "### Test scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "706b7979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/6jg25ryd59n_ggsp1fp4y47r0000gn/T/ipykernel_4279/1064654185.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  scores[i] = cosine_similarity(user_embedding.detach().numpy().reshape(1, -1), senator_embedding.detach().numpy().reshape(1, -1))\n",
      "/var/folders/s1/6jg25ryd59n_ggsp1fp4y47r0000gn/T/ipykernel_4279/1064654185.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  scores[i] = cosine_similarity(user_embedding.detach().numpy().reshape(1, -1), senator_embedding.detach().numpy().reshape(1, -1))\n",
      "/var/folders/s1/6jg25ryd59n_ggsp1fp4y47r0000gn/T/ipykernel_4279/1064654185.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  scores[i] = cosine_similarity(user_embedding.detach().numpy().reshape(1, -1), senator_embedding.detach().numpy().reshape(1, -1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.84016222, 0.83779895, 0.83383113])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate senators_embeddings\n",
    "senators_embeddings = senators_test_df['initiatives_summary_dummy'].apply(lambda x: generate_embeddings(x, tokenizer, model))\n",
    "\n",
    "# Get the user's input\n",
    "user_input = \"Quiero proteccion para los animales\"\n",
    "\n",
    "# Match the senators\n",
    "similarity_scores = match_senators(user_input, senators_embeddings, tokenizer, model)\n",
    "similarity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599427a",
   "metadata": {},
   "source": [
    "## Get top senators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a325a62",
   "metadata": {},
   "source": [
    "### Function to get the senator with best matching score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2598aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_senators(scores, senators_df, N=5):\n",
    "    # Get the indices of the senators sorted by their scores\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    \n",
    "    # Get the top N senators\n",
    "    top_senators = senators_df.iloc[sorted_indices[:N]]\n",
    "    \n",
    "    return top_senators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2863f3",
   "metadata": {},
   "source": [
    "### Test top senators function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a044c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senator: Jos√© Alfredo Botello Montes, Score: 0.8401622176170349\n",
      "Senator: Estrella Rojas Loreto, Score: 0.8377989530563354\n",
      "Senator: Roberto Juan Moya Clemente, Score: 0.8338311314582825\n"
     ]
    }
   ],
   "source": [
    "# Get the top 5 senators\n",
    "top_senators = get_top_senators(similarity_scores, senators_test_df)\n",
    "\n",
    "# Print the names and scores of the top senators\n",
    "for i, row in top_senators.iterrows():\n",
    "    print(f\"Senator: {row['Nombre']} {row['Apellidos']}, Score: {similarity_scores[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
